{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replication of Community_study.ipynb (Fabrice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/') # adding path to use consensus clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import community as community_louvain\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "import nilearn\n",
    "from nilearn.datasets import fetch_atlas_aal\n",
    "import nilearn.plotting as plotting\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the subject data\n",
    "G = nx.convert_matrix.from_numpy_matrix(loadmat('../Data/sub1_SC.mat')['sub1_SC'])\n",
    "# draw the graph\n",
    "# Important: call this one only once, so that node positions are fixed afterwards \n",
    "# (The spring representation does not necessarily reach a global extremum and might lead to visually \n",
    "# different graph orientations and confusion conclusions)\n",
    "pos = nx.spring_layout(G, scale=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "# compute the best partition\n",
    "partition = community_louvain.best_partition(G, weight='weight', resolution=0.5)\n",
    "\n",
    "# color the nodes according to their partition\n",
    "cmap = cm.get_cmap('hsv', max(partition.values()) + 1)\n",
    "nx.draw_networkx_nodes(G, pos, partition.keys(), node_size=40,\n",
    "                       cmap=cmap, node_color=list(partition.values()))\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brain visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aal = fetch_atlas_aal()\n",
    "# Get coordinates of the regions in AAL atlas and their corresponding labels\n",
    "coordinates, label_list = plotting.find_parcellation_cut_coords(labels_img=aal['maps'], return_label_names=True) # Note that we compute coordinates for all 116 nodes here, but it doesn't really matter\n",
    "\n",
    "# Re-order coordinates in sorted order of labels, so that they match with the original atlas' order\n",
    "coords = coordinates[np.argsort(label_list)]\n",
    "\n",
    "# We only consider the first 90 regions and ignore cerebellum\n",
    "limit = 90\n",
    "\n",
    "def plot_markers_based_on_partition(coords, partition, cmap, output_name='community_example.html'):\n",
    "    \"\"\"\n",
    "    Given markers (defined by their coordinates) as well as a color map function and a partition vector, plot in \n",
    "    interactive 3D plot markers in MNI space, overlaid on a glass brain.\n",
    "    The visualization is saved in the Results subdirectory as an HTML file with the provided name.\n",
    "    :param coords: 3D coordinates of each marker in MNI space (should be N x 3, where N the number of markers)\n",
    "    :param partition: Nx1 vector of assignments, denoting for each marker its community\n",
    "    :param cmap: Colormap function\n",
    "    :param output_name: Name under which to save visualization. (Default: community_example.html)\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    # Plot first 90 region coordinates. Each node is colored according to its community value\n",
    "    view = plotting.view_markers(coords, cmap(partition)) \n",
    "    view.save_as_html('../Results/' + output_name)\n",
    "    view.open_in_browser()\n",
    "    return view\n",
    "    \n",
    "plot_markers_based_on_partition(coords[:limit], list(partition.values())[:limit], cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consensus Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from consensus_clustering import pairwise_accord_naive, are_equal_up_to_bijection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_consensus_from_graph(graph, number_partitions, threshold, resolution):\n",
    "    \"\"\"\n",
    "    Applies Louvain algorithm number_partitions times, with provided resolution.\n",
    "    For each partition, compute the pairwise accordance (ie: if two nodes are grouped in same community or not), and constitute consensus matrix as the sum of these accordance matrices\n",
    "    The consensus matrix is then normalized by the total number of partitions and thresholded by the provided threshold value.\n",
    "    \n",
    "    :param graph: The graph on which to apply the algorithm\n",
    "    :param number_partitions: Number of different partitions to compute\n",
    "    :param threshold: Value with which to threshold the consensus matrix, setting all entries below threshold to zero\n",
    "    :param resolution: Resolution to use in the Louvain algorithm\n",
    "    \n",
    "    :return consensus_matrix, the consensus matrix obtained by the procedure\n",
    "    :return partitions, the number_partitions partitions obtained by the procedure\n",
    "    \"\"\"\n",
    "    n_nodes = len(list(graph.nodes))\n",
    "    consensus_matrix = np.zeros((n_nodes, n_nodes))\n",
    "    partitions = np.zeros((number_partitions, n_nodes))\n",
    "    # First get consensus matrix\n",
    "    for i in range(0, number_partitions):\n",
    "        partition = community_louvain.best_partition(graph, weight='weight', resolution=resolution)\n",
    "        partitions[i, :] =  np.asarray(list(partition.values()))\n",
    "        consensus_matrix += pairwise_accord_naive(partitions[i, :])\n",
    "    consensus_matrix /= number_partitions\n",
    "    # Threshold\n",
    "    consensus_matrix[consensus_matrix < threshold] = 0.0\n",
    "    return consensus_matrix, partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consensus_clustering(graph, number_partitions, threshold, n_steps, resolution):\n",
    "    \"\"\"\n",
    "    Overall consensus clustering algorithm described in https://www.nature.com/articles/srep00336#rightslink\n",
    "    A first consensus matrix is computed on the original graph.\n",
    "    Then this consensus matrix is treated as an adjacency matrix itself, on which we apply the consensus procedure until either\n",
    "    convergence (meaning all partitions are the same) or maximum number of steps are reached.\n",
    "\n",
    "    :param graph: The original graph on which to perform consensus clustering\n",
    "    :param number_partitions: Number of partitions at each iteration of the clustering algorithm\n",
    "    :param threshold: The threshold used in consensus matrix computation\n",
    "    :param n_steps: The maximum number of steps before termination of the algorithm, should it fail to reach convergence\n",
    "    :param resolution: The resolution for Louvain's algorithm\n",
    "\n",
    "    :return partitions: The last number_partitions derived by the algorithm. If algorithm is converged, they are all equal up to bijection.\n",
    "    :return i: The iteration at which the algorithm finished. Useful to assess how quickly it converged or if it even converged at all.\n",
    "    \"\"\"\n",
    "    consensus_matrix, partitions = compute_consensus_from_graph(graph, number_partitions, threshold, resolution)\n",
    "\n",
    "    # Until convergence or number of steps exceeded\n",
    "    for i in range(0, n_steps):\n",
    "        # Convert consensus matrix to graph\n",
    "        G = nx.convert_matrix.from_numpy_matrix(consensus_matrix)\n",
    "        # Get new consensus matrix\n",
    "        consensus_matrix, partitions = compute_consensus_from_graph(G, number_partitions, threshold, resolution)\n",
    "\n",
    "        # Now we must ask if all category vectors are the same or not\n",
    "        should_stop = True\n",
    "        for vi in range(0, number_partitions - 1):\n",
    "            should_stop = are_equal_up_to_bijection(partitions[vi], partitions[vi + 1])\n",
    "            if not should_stop:\n",
    "                break\n",
    "        if should_stop:\n",
    "            break\n",
    "    return partitions, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions, i = consensus_clustering(G, 100, 0.1, 500, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = partitions[0].astype(int)\n",
    "cmap = cm.get_cmap('hsv', max(p) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_markers_based_on_partition(coords[:limit], p, cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional graph definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FC = loadmat('../Data/sub1_Motor.mat')['sub1_Motor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearsonr(x):\n",
    "    \"\"\"\n",
    "    Simply computes correlation between all pairs of lines in X.\n",
    "    It is assumed that X is in the form variables x observations.\n",
    "    If X is in observations x variables it should be transposed before being passed to this function!\n",
    "    \"\"\"\n",
    "    return np.corrcoef(x, rowvar=True)\n",
    "\n",
    "def sliding_window_corr(x, window_length, window_stride, window_function):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    half_window=window_length//2\n",
    "    window_f = window_function(window_length) # This serves to taper the samples\n",
    "    start = half_window\n",
    "    end = x.shape[0] - half_window\n",
    "    \n",
    "    n_windows = (end-start)//window_stride + 1 # Check that this is correct\n",
    "    \n",
    "    print('There should be {} windows'.format(n_windows))\n",
    "    print('Start: {} - End: {}'.format(start, end))\n",
    "    \n",
    "    correlations = np.zeros((n_windows,x.shape[1], x.shape[1]))\n",
    "    for i in range(0, n_windows):\n",
    "        # The window is centered at i*window_stride+start\n",
    "        center = i*window_stride + start\n",
    "        # Left side is center - half_window\n",
    "        # Right side is center + half_window\n",
    "        # We compute correlation of current sample tampered with window function\n",
    "        correlations[i]=pearsonr(x[center-half_window:center+half_window].T*window_f)\n",
    "    return correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = sliding_window_corr(FC, 60, 1, lambda x: np.hamming(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure=plt.figure(dpi=100)\n",
    "\n",
    "plt.imshow(corrs[10,:,:], cmap=\"jet\")\n",
    "plt.show()\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_communities_from_partition(G, partition):\n",
    "    tmp_G = G.copy()\n",
    "    unique= np.unique(partition)\n",
    "    for i in range(0, p.size):\n",
    "        tmp_G.add_nodes_from([i], partition=p[i])\n",
    "    subgraphs = [tmp_G.subgraph((node for node, data in tmp_G.nodes(data=True) if data.get(\"partition\") == e)) for i, e in enumerate(unique)]\n",
    "    if not nx.community.is_partition(tmp_G, subgraphs):\n",
    "        raise AssertionError('The provided partition is not a proper partition for this graph')    \n",
    "    return subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign to every node in the graph its community\n",
    "subgraphs = create_communities_from_partition(G, p)\n",
    "\n",
    "# The graph G should not have a partition field, because the method performs its operations on a copy of G\n",
    "G.nodes[0]\n",
    "\n",
    "# Now, we are ready to compute some informations on each community. In particular, we can compute for each node its clustering coefficient within its community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segregation & Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "\n",
    "def compute_degree(G, standardize=True, weighted=True):\n",
    "    \"\"\"\n",
    "    Function to compute degree of all nodes within the graph of interest.\n",
    "    If weighted degree is requested, the degree will then be defined as the sum of weighted edges, for each node.\n",
    "    If z-scoring is requested, the function will standardize the returned degrees.\n",
    "    \n",
    "    :param G: input graph. Degree will be computed for each node within this graph\n",
    "    :param standardize: Whether degrees should be standardized before being returned. (Default = True)\n",
    "    :param weighted: Computes weighted degree instead of regular degree. If the graph is unweighted, this parameter is ignored. (Default = True)\n",
    "    \n",
    "    :return nodes: Nodes of the graph (for debug purposes)\n",
    "    :return degrees: Degrees computed according to the chosen method.\n",
    "    \"\"\"\n",
    "    if weighted and nx.is_weighted(G):\n",
    "        nodes = list(G.nodes)\n",
    "        degrees = [0]*len(nodes)\n",
    "        i = 0\n",
    "        for n in nodes:\n",
    "            for neighbor in G[n]:\n",
    "                degrees[i] += G[n][neighbor]['weight']\n",
    "            i+=1\n",
    "    else:\n",
    "        nodes,degrees=zip(*list(G.degree))\n",
    "    \n",
    "    degrees = np.asarray(degrees)\n",
    "    if standardize:\n",
    "        degrees = (degrees-degrees.mean())/degrees.std()\n",
    "    return nodes, degrees\n",
    "\n",
    "def compute_participation_coefficient(G, weighted, partition_values):\n",
    "    \"\"\"\n",
    "    Computes participation coefficient as 1 - sum((k_is / k_i)**2), where k_is is the degree of node i in community s\n",
    "    and k_i is the degree of node i in the graph (thus, sum of k_is).\n",
    "    \n",
    "    :param G: graph on which to compute participation coefficient node-wise.\n",
    "    :param weighted: Whether to compute weighted degree or not.\n",
    "    :param partition_values: Vector indicating which community which node belongs to.\n",
    "    \n",
    "    :return participation coefficient vector of shape n_nodes x 1\n",
    "    \"\"\"\n",
    "    nodes = list(G.nodes)\n",
    "    n_nodes = len(nodes)\n",
    "    partition_categories = np.unique(partition_values)\n",
    "    degrees = np.zeros((n_nodes, partition_categories.size))\n",
    "    for n_i in range(0, n_nodes):\n",
    "        for c_i, u in enumerate(unique_values):\n",
    "            neighbours = G[n_i]\n",
    "            for neighbour in neighbours:\n",
    "                if p[neighbour] == u:\n",
    "                    if weighted:\n",
    "                        degrees[n_i, c_i] += neighbours[neighbour]['weight']\n",
    "                    else:\n",
    "                        degrees[n_i, c_i] += 1\n",
    "    norm_factor = 1./ degrees.sum(axis=1)\n",
    "    s = (norm_factor.reshape((-1,1)) * degrees)**2\n",
    "    return 1-s.sum(axis=1)\n",
    "    #return degrees\n",
    "    \n",
    "\n",
    "def compute_system_segregation(G, partition_values):\n",
    "    \"\"\"\n",
    "    System segregation is defined as: \n",
    "    (mean(connections within same community) - mean(connections not part of same community))/mean(connection within same community)\n",
    "    \"\"\"\n",
    "    # First, compute all communities\n",
    "    subgraphs = create_communities_from_partition(G, partition_values)\n",
    "    \n",
    "    # Next for all communities, compute the within weighted degree values\n",
    "    degree_values = [compute_degree(subgraph,standardize=False, weighted=True)[1] for subgraph in subgraphs]\n",
    "    \n",
    "    # Now we must fuse all these together\n",
    "    degree_values = reduce(lambda x,y: np.hstack((x, y)), degree_values)\n",
    "\n",
    "    mean_same_community_weight = degree_values.mean()\n",
    "    \n",
    "    # Now, we can worry about node pairs, ie: edges!\n",
    "    edges = list(G.edges)\n",
    "    count=0\n",
    "    mean_diff_community_weight = 0;\n",
    "    for e in edges:\n",
    "        n1, n2 = e[0], e[1]\n",
    "        if partition_values[n1] != partition_values[n2]:\n",
    "            count += 1\n",
    "            mean_diff_community_weight += G[n1][n2]['weight']\n",
    "    mean_diff_community_weight /= count\n",
    "    #return mean_same_community_weight\n",
    "    return (mean_same_community_weight - mean_diff_community_weight)/mean_same_community_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_system_segregation(G, p).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
